{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e15fbd7",
      "metadata": {
        "id": "6e15fbd7"
      },
      "source": [
        "# OCR For Text Extraction\n",
        "---\n",
        "## 1. Installing Packages & Import Modules\n",
        "Packages required to do text extraction are:\n",
        "- **DocTR [TensorFlow]**\n",
        ": OCR logic handler, used for extracting textual information from document or images. Supported in TensorFlow2\n",
        "- **PyPDF2**\n",
        ": PDF document handler, capable of retrieve text and metadata from PDFs\n",
        "- **PyFDPF**\n",
        ": PyFPDF is a library for PDF document generation under Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578a5b47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "578a5b47",
        "outputId": "693ec782-c9bc-434d-cf13-46f707592998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-doctr[tf]\n",
            "  Downloading python_doctr-0.6.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata (from python-doctr[tf])\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (1.22.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (1.10.1)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (4.7.0.72)\n",
            "Collecting pypdfium2<4.0.0,>=3.0.0 (from python-doctr[tf])\n",
            "  Downloading pypdfium2-3.21.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyclipper<2.0.0,>=1.2.0 (from python-doctr[tf])\n",
            "  Downloading pyclipper-1.3.0.post4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (813 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.9/813.9 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0,>=1.6.0 (from python-doctr[tf])\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect<2.0.0,>=1.0.9 (from python-doctr[tf])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (3.7.1)\n",
            "Collecting weasyprint>=55.0 (from python-doctr[tf])\n",
            "  Downloading weasyprint-59.0-py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.6/267.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (8.4.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (0.7.1)\n",
            "Collecting mplcursors>=0.3 (from python-doctr[tf])\n",
            "  Downloading mplcursors-0.5.2.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.0/89.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.0.0 (from python-doctr[tf])\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (4.65.0)\n",
            "Collecting rapidfuzz>=1.6.0 (from python-doctr[tf])\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from python-doctr[tf])\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[tf]) (2.12.0)\n",
            "Collecting tensorflow-addons>=0.17.1 (from python-doctr[tf])\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf2onnx>=1.9.2 (from python-doctr[tf])\n",
            "  Downloading tf2onnx-1.14.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->python-doctr[tf]) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect<2.0.0,>=1.0.9->python-doctr[tf]) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->python-doctr[tf]) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.54.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.32.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.17.1->python-doctr[tf])\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx>=1.9.2->python-doctr[tf])\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers>=2.0 (from tensorflow<3.0.0,>=2.9.0->python-doctr[tf])\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pydyf>=0.6.0 (from weasyprint>=55.0->python-doctr[tf])\n",
            "  Downloading pydyf-0.6.0-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.10/dist-packages (from weasyprint>=55.0->python-doctr[tf]) (1.15.1)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from weasyprint>=55.0->python-doctr[tf]) (1.1)\n",
            "Requirement already satisfied: tinycss2>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from weasyprint>=55.0->python-doctr[tf]) (1.2.1)\n",
            "Collecting cssselect2>=0.1 (from weasyprint>=55.0->python-doctr[tf])\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Collecting Pyphen>=0.9.1 (from weasyprint>=55.0->python-doctr[tf])\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=8.3.2 (from python-doctr[tf])\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->python-doctr[tf]) (3.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.40.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=0.6->weasyprint>=55.0->python-doctr[tf]) (2.21)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2>=0.1->weasyprint>=55.0->python-doctr[tf]) (0.5.1)\n",
            "Collecting zopfli>=0.1.4 (from fonttools>=4.22.0->matplotlib>=3.1.0->python-doctr[tf])\n",
            "  Downloading zopfli-0.2.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (848 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.9/848.9 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.0.1 (from fonttools>=4.22.0->matplotlib>=3.1.0->python-doctr[tf])\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->python-doctr[tf]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->python-doctr[tf]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->python-doctr[tf]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->python-doctr[tf]) (3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3.0.0,>=2.9.0->python-doctr[tf]) (3.2.2)\n",
            "Building wheels for collected packages: langdetect, mplcursors\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=b890af8caed7924cf7cf3a3366dce9a465ee96c3ade0f5eefa3ad6bef847b2f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for mplcursors (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mplcursors: filename=mplcursors-0.5.2-py3-none-any.whl size=21054 sha256=428f26a637f96405aa60409b942ef14058378bdb42ef2ee84c11d1f2d3b58205\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/5b/fb/aed35cc15262c380536820fa3cb2e2d41fb52450de918a6785\n",
            "Successfully built langdetect mplcursors\n",
            "Installing collected packages: pyclipper, flatbuffers, brotli, zopfli, unidecode, typeguard, shapely, rapidfuzz, Pyphen, pypdfium2, pydyf, Pillow, onnx, langdetect, importlib-metadata, tf2onnx, tensorflow-addons, huggingface-hub, cssselect2, weasyprint, mplcursors, python-doctr\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed Pillow-9.5.0 Pyphen-0.14.0 brotli-1.0.9 cssselect2-0.7.0 flatbuffers-2.0.7 huggingface-hub-0.15.1 importlib-metadata-6.6.0 langdetect-1.0.9 mplcursors-0.5.2 onnx-1.14.0 pyclipper-1.3.0.post4 pydyf-0.6.0 pypdfium2-3.21.1 python-doctr-0.6.0 rapidfuzz-3.1.1 shapely-1.8.5.post1 tensorflow-addons-0.20.0 tf2onnx-1.14.0 typeguard-2.13.3 unidecode-1.3.6 weasyprint-59.0 zopfli-0.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=9c0fdc32f66cc082a0d5dafce950f03df3fc705d76d51ae192c9ae258f1b1976\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rapidfuzz==2.15.1\n",
            "  Downloading rapidfuzz-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "  Attempting uninstall: rapidfuzz\n",
            "    Found existing installation: rapidfuzz 3.1.1\n",
            "    Uninstalling rapidfuzz-3.1.1:\n",
            "      Successfully uninstalled rapidfuzz-3.1.1\n",
            "Successfully installed rapidfuzz-2.15.1\n"
          ]
        }
      ],
      "source": [
        "# Main Libs\n",
        "!pip install python-doctr[tf]\n",
        "!pip install PyPDF2\n",
        "!pip install fpdf\n",
        "\n",
        "# Downgraded Libs\n",
        "!pip install rapidfuzz==2.15.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78b991a",
      "metadata": {
        "id": "c78b991a"
      },
      "source": [
        "**UserWarning** \\\n",
        "If warning message pops up, just ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e1315",
      "metadata": {
        "id": "f53e1315",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import PyPDF2 as pypdf\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837cca94",
      "metadata": {
        "id": "837cca94"
      },
      "source": [
        "## 2. Building Models\n",
        "So to make OCR works, we need to combine text detection model and text recognition model as an OCR pipeline to recognize text characters.\n",
        "\n",
        "### Text Detection Model\n",
        "For the text recognition model, we will use **DB ResNet50**. From the sources that I have read, this model somehow very popular among other document OCR apps for text detection model. It's also recommended by DocTR to use this model instead of the other models. Further research may be needed to understand why this is used.\n",
        "\n",
        "### Text Recognition Model\n",
        "For the text recognition model, we will use **CRNN VGG-16 Backbone**. Same as ResNet50, this model somehow very popular among other OCR apps for text recognition model. It's also recommended by DocTR to use this model instead of the other models. Further research may needed to understand why this is used.\n",
        "\n",
        "**Status:** \\\n",
        "Combining both of these models is somewhat complicated. I have built the models from my local environment, still have no idea how to export it for passing DocTR ocr_predictor arguments. Right now, we will use pretrained model from DocTR itself so it doesn't crash while passing ocr_predictor args. The result from self trained model and pretrained model are nearly the same anyway, so don't worry about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c06ff80",
      "metadata": {
        "id": "8c06ff80"
      },
      "outputs": [],
      "source": [
        "# this block of code will be updated as soon as my self trained models works properly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3fed1d",
      "metadata": {
        "id": "4b3fed1d"
      },
      "source": [
        "## 3. Extract Text using Trained OCR Models with DocTR\n",
        "To extract text with DocTR, we have to use built in ocr_predictor function from DocTR. This function allow us to use self trained or pretrained models for text detection and text recognition. The ocr_predictor returns a document object with a nested structure (with Page, Block, Line, Word, Artefact)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce830b0d",
      "metadata": {
        "id": "ce830b0d"
      },
      "outputs": [],
      "source": [
        "def extract_with_ocr(file):\n",
        "    # pretrained model\n",
        "    model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)\n",
        "\n",
        "    # reading files\n",
        "    document = DocumentFile.from_pdf(file)\n",
        "\n",
        "    # analyze|\n",
        "    result = model(document)\n",
        "\n",
        "    # export to json\n",
        "    output = result.export()\n",
        "\n",
        "    # grouping detected words\n",
        "    separated_words = []\n",
        "    for page in output[\"pages\"]:\n",
        "        for block in page[\"blocks\"]:\n",
        "            for line in block[\"lines\"]:\n",
        "                for word in line[\"words\"]:\n",
        "                    separated_words.append(word[\"value\"])\n",
        "\n",
        "    # combining separated words into sentences\n",
        "    output = \" \".join(separated_words)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5200f3e",
      "metadata": {
        "id": "e5200f3e"
      },
      "source": [
        "The function below is needed to check if the pdf file is a scanned pdf (image based) or digital pdf (text based). If the function didn't detect any characters, it will be considered scanned pdf, so it had to handle extracting text using OCR. If the function detected some characters (100 chars for example) in the pdf file, then it will be considered digital pdf, so it just need to extract text directly from PyPDF2 for better accuracy.\n",
        "\n",
        "This function return strings of extracted text from the pdf file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13ecb70",
      "metadata": {
        "id": "d13ecb70"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def text_extractor(file):\n",
        "    reader = pypdf.PdfReader(file)\n",
        "    page = reader.pages[0]\n",
        "\n",
        "    chars_count = 100\n",
        "    if len(page.extract_text()) > chars_count:\n",
        "        text = page.extract_text()\n",
        "\n",
        "        clean_text = re.sub(r'[^\\w\\ \\n]', '', text)\n",
        "\n",
        "        return clean_text\n",
        "    else:\n",
        "        return extract_with_ocr(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1efbd549",
      "metadata": {
        "id": "1efbd549"
      },
      "source": [
        "## 4. Let's Test It\n",
        "This funtion will print out the extracted text from your document. You can play around with any pdf file you want to extract text from. Just change the file_name value to your pdf document path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below if you want to run it with OCR directly"
      ],
      "metadata": {
        "id": "9qCMVKWGvsb3"
      },
      "id": "9qCMVKWGvsb3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f504b96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "3f504b96",
        "outputId": "ea752fd7-de10-4f77-9054-ee1caa8e4464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanned PDF extracted text example:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Muhammad Alfian Pratama LinkedIn I +62-855-2078-1007 I e alfianp613.github.io I M alfianp613@gmail.com OGitHub I\\'m a 6th semester student curious and interested in Data Science and Machine Learning. I possess advanced proficiency in Python and R programming languages, and Ihave honed my skills in prominent frameworks such as TensorFlow and Flask. lam currently seeking an opportunity to expand and apply my skills through a one semester industry placement, with a particular focus on data-related roles. I am eager to delve deeper into the practical aspects of the field and gain invaluable real-world experience. Skills Education Python I R I HTML I CSS I Javascript I Tableau I Microsoft Excel I Flask I Tensorflow I SPSS I Minitab I MySQL I NoSQL I Firebase Machine Learning I Data Science I Data Analytics I Statistics I Microservices I Backend I English Machine Learning Learning Path Student with Ahead of Schedule Status Specialization Bachelor of Data Science Major in Data Science Technology (GPA 3.88/4). 3rd most outstanding FTMM Universitas Airlangga Student Databases, Spatial Data Analysis. Work Experience Laboratory Assistant Projects Bangkit Academy 2023 By Google, GoTo, & Traveloka Indonesia 02/2023 - Current \"Magang dan Studi Independen Bersertifikat\" Batch 4 held by Kemendikbud RI Accomplish 6 Specialization Courses from Coursera such as Google Data Analytics, Google IT Automation, Mathematics for Machine Learning Specialization, Machine Learning Specialization, DeepLearning.Al TensorFlow Developer Specialization, and TensorFlow: Data and Deployment Universitas Airlangga Surabaya, Indonesia 09/2020 - Current Related Courses: Programming Algorithm, Calculus, Linear Algebra, Parametric Statistics, Non Statistics, Probability, Computational Statistics, Mathematical Statistics, Multivariate Statistics, Stochastic Process, Survival Analysis, Data Mining, Natural Language Processing, FTMM Universitas Airlangga Surabaya, Indonesia 03/2022 - 07/2022 Laboratory Assistant for Programming Algorithm Assisted Lecturer for scoring practice modules and mentoring for 25 students Bicara Pilpres (Sentiment Analysis Dashboard for Indonesian Presidential Surabaya, Indonesia 12/2022 - 01/2023 Election 2024 Candidates) Firebase Storage. Led the end-to-end development process such as front-end design, software architecture, machine learning model, pipeline. Successfully implemented a microservices architecture, utilizing Python Flask for the main website backend and machine learning API, to enhance the scalability and efficiency of the web application. Also, implemented Firebase to store data using Firebase Firestore Database (NOSQL) and Implemented deployment of the web application on Digital Ocean Droplets, configuring SSL certification and domain integration using Nginx. Achieved secure and reliable web hosting, ensuring optimal performance and user experience. Implemented Progressive Web Application (PWA) for better user experience. This project got Top 50 Hackfest 2023 held by GDSC Indonesia. SIBI (Sistem Bahasa Isyarat Indonesai) Sign Language Alphabetic Implemented Tensorflow Data Generator for augmenting data to reproduce primary data. Achieving 94% accuracy on training data and 90% accuracy on testing data. Surabaya, Indonesia 11/2022 - 12/2022 Classification using Mediapipe Utilized the power of Mediapipe to effectively extract hand landmark points, enabling the training of a cutting-edge machine learning model and the development of real-time detection capabilities. Organizational Experience, Leader GDSC Universitas Airlangga BEM FTMM Universitas Airlangga Surabaya, Indonesia 09/2022 - Current Surabaya, Indonesia 01/2022 - 01/2023 Surabaya, Indonesia 09/2021 - 07/2022 Surabaya, Indonesia 09/2021 - 07/2022 Led a core team of 16 people and engage 500+ member. Held 10 workshops on data science & flutter with an average of 50+ participants. Minister of Science, Research, and Led two sub division with 5 member each sub division. Held 8 event impacting 400+ FTMM students and 200+ external students. Technology Core Team Mentor Staff Ministry of Science and Technology GDSC Universitas Airlangga BEM Universitas Airlangga Be a speaker at the webinar event Info session GDSC UNAIR with the topic Data Science 101 with 100+ participants. Made a Syllabus for workshop, especially workshops that related to Data Science or Machine Learning. Organized Airlangga Scientific Writing Workshop and Competition, impacting 100+ students. Organized Airlangga Scientific Forum with 32 representative participants from every faculty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# test scanned pdf\n",
        "print(\"Scanned PDF extracted text example:\")\n",
        "file_name = \"Muhammad Alfian Pratama new resume.pdf\" # select path to your pdf document in your local environment\n",
        "output = extract_with_ocr(file_name)\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below if you want to run it and let the code decide wether it should be extracted with PyPDF2 or DocTR"
      ],
      "metadata": {
        "id": "nRafPLdLvuU1"
      },
      "id": "nRafPLdLvuU1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71551b06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "71551b06",
        "outputId": "f03d4304-007e-4cd1-a90d-e7e305dffc45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unconfirmed PDF extracted text example:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Muhammad Alfian Pratama  \\n LinkedIn     628552078 1007      alfianp613githubio      alfianp613gmailcom       GitHub  \\nIm a 6th semester student curious and interested in Data Science and Machine Learning  I possess advanced proficiency in Python and \\nR programming languages and I have honed my skills in prominent frameworks such as TensorFlow and Flask I am currently seek ing an \\nopportunity to expand and apply my skills through a one semester industry plac ement with a particular focus on data related roles I \\nam eager to delve deeper into the practical aspects of the field and gain invaluable real world experience  \\n \\nSkills  ____________________________________________________________________________________ ________ ___  \\n \\n   Python  R  HTML  CSS  Javascript  Tableau  Microsoft Excel  Flask  Tensorflow  SPSS  Minitab  MySQL  NoSQL  Fire base  \\n   Machine Learning  Data Science  Data Analytics  Statistics  Microservices  Backend  English  \\nEducation  _________ ______________________________________________________________________________ ___ \\nMachine Learning Learning Path   Bangkit  Academy 2023 By Google \\nGoTo  Traveloka  Indonesia  022023  Current  \\n Magang dan Studi Independen Bersertifikat Batch 4 held by Kemendikbud RI  \\n Student with Ahead of Schedule Status  \\n Accomplish 6 Specialization Courses from Coursera such as Google Data Analytics Google IT Automation Mathematics for Machin e Learning \\nSpecialization Machine Learning Specialization DeepLearningAI TensorFlow Developer Specialization and TensorFlow Da ta and Deployment \\nSpecialization  \\nBachelor of Data Science   Universitas Airlangga  Surabaya  Indonesia  092020  Current  \\n Major in Data Science Technology  GPA 3884  \\n 3rd most outstanding FTMM Universitas Airlangga Student  \\n Related Courses Programming Algorithm Calculus Linear Algebra Parametric Statistics Non Parametric Statistics Probabilit y Computational \\nStatistics  Mathematical Statistics Multivariate Statistics Stochastic Process Survival Analysis Data Min ing Natural Language Processing \\nDatabases Spatial Data   Analysis  \\nWork Experience  ____________________________________________________________________________________  \\nLaboratory Assistant   FTMM Universitas Airlangga  Surabaya  Indonesia  032022  072022  \\n Laboratory Assistant for Programming Algorithm course s \\n Assisted Lecturer for scoring practice modules and mentoring for 25 students  \\n \\nProjects  _______ __________________________________________________________________________________ ___ \\nBicara Pilpres  Sentiment Analysis Dashboard for Indonesian Presidential \\nElection 2024 Candidates  Surabaya Indonesia  122022  012023  \\n Led the endtoend development process such as front end design software architecture machine learning model pipeline  \\n Successfully implemented a microservices architecture utilizing Python Flask for the main website backend and machine learni ng API to enhanc e \\nthe scalability and efficiency of the web application  Also implemented Firebase to store data using Firebase Firestore Database NoSQL and \\nFirebase Storage  \\n Implemented deployment of the web application on Digital Ocean Droplets configuring SSL certi fication and domain integration using Nginx  \\nAchieved secure and reliable web hosting ensuring optimal performance and user experience  \\n Implemented Progressive Web Application PWA for better user experience  \\n This project got Top 50 Hackfest 2023 held by  GDSC Indonesia  \\nSIBI Sistem Bahasa Isyarat  Indonesai Sign Language Alphabetic \\nClassification using Mediapipe  Surabaya Indonesia  112022  122022  \\n Implemented Tensorflow Data Generator for augmenting data to reproduce primary data  \\n Utilized the power of Mediapipe to effectively extract hand landmark points enabling the training of a cutting edge machine learning model and \\nthe development of real time detection capabilities  \\n Achieving 94 accuracy on training data and 90 accuracy on  testing data  \\nOrganizational Experience ____________________________________________________________________________  \\nLeader   GDSC Universitas Airlangga  Surabaya Indonesia  092022  Current  \\n Led a core team of 16 people and engage 500 member  \\n Held 10 workshops on data science  flutter with an average of 50 participants  \\nMinister of Science  Research and \\nTechnology   BEM FTMM Universitas \\nAirlangga  Surabaya Indonesia  012022  012023  \\n Led two sub division with 5 member each sub division  \\n Held 8 event impacting 400 FTMM students and 200 external students  \\nCore Team Mentor   GDSC Universitas Airlangga  Surabaya Indonesia  092021  072022  \\n Be a speaker at the webinar event Info session GDSC UNAIR with the topic Data Science 101 with 100 participants  \\n Made a Syllabus for workshop especially workshops that related to Data Science or Machine Learning  \\nStaff Ministry of Science \\nand Technology   BEM Universitas Airlangga  Surabaya Indonesia  092021  072022  \\n Organized Airlangga Scientific Writing Workshop and Competition impacting 100 students  \\n Organized Airlangga Scientific Forum with 32 representative participants from every faculty  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# test unconfirmed pdf\n",
        "print(\"Unconfirmed PDF extracted text example:\")\n",
        "file_name = \"Muhammad Alfian Pratama new resume.pdf\" # select path to your pdf document in your local environment\n",
        "output = text_extractor(file_name)\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Exporting String Output into PDF\n",
        "Needed for CV Summarization"
      ],
      "metadata": {
        "id": "I7QBB_VFiK7A"
      },
      "id": "I7QBB_VFiK7A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa7712a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4aa7712a",
        "outputId": "73014d57-53b2-406d-ff33-9de7da580bd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "# save FPDF() class into a\n",
        "# variable pdf\n",
        "pdf = FPDF()\n",
        "# Add a page\n",
        "pdf.add_page()\n",
        "\n",
        "# set style and size of font\n",
        "# that you want in the pdf\n",
        "pdf.set_font(\"Arial\", size = 12)\n",
        "\n",
        "# create a cell\n",
        "pdf.cell(200, 10, txt = output,\n",
        "         ln = 1, align = 'J')\n",
        "\n",
        "# save the pdf with name .pdf\n",
        "pdf.output(\"temp.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test temp pdf\n",
        "file_name = \"temp.pdf\" # select path to your pdf document in your local environment\n",
        "output = text_extractor(file_name)\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "2Y95efSV19tQ",
        "outputId": "6516372d-3cce-4e2a-e536-ff37ca12179f"
      },
      "id": "2Y95efSV19tQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Muhammad Alfian Pratama  \\n LinkedIn     628552078 1007      alfianp613githubio      alfianp613gmailcom       GitHub  \\nIm a 6th semester student curious and interested in Data Science and Machine Learning  I possess advanced proficiency in Python and \\nR programming languages and I have honed my skills in prominent frameworks such as TensorFlow and Flask I am currently seek ing an \\nopportunity to expand and apply my skills through a one semester industry plac ement with a particular focus on data related roles I \\nam eager to delve deeper into the practical aspects of the field and gain invaluable real world experience  \\n \\nSkills  ____________________________________________________________________________________ ________ ___  \\n \\n   Python  R  HTML  CSS  Javascript  Tableau  Microsoft Excel  Flask  Tensorflow  SPSS  Minitab  MySQL  NoSQL  Fire base  \\n   Machine Learning  Data Science  Data Analytics  Statistics  Microservices  Backend  English  \\nEducation  _________ ______________________________________________________________________________ ___ \\nMachine Learning Learning Path   Bangkit  Academy 2023 By Google \\nGoTo  Traveloka  Indonesia  022023  Current  \\n Magang dan Studi Independen Bersertifikat Batch 4 held by Kemendikbud RI  \\n Student with Ahead of Schedule Status  \\n Accomplish 6 Specialization Courses from Coursera such as Google Data Analytics Google IT Automation Mathematics for Machin e Learning \\nSpecialization Machine Learning Specialization DeepLearningAI TensorFlow Developer Specialization and TensorFlow Da ta and Deployment \\nSpecialization  \\nBachelor of Data Science   Universitas Airlangga  Surabaya  Indonesia  092020  Current  \\n Major in Data Science Technology  GPA 3884  \\n 3rd most outstanding FTMM Universitas Airlangga Student  \\n Related Courses Programming Algorithm Calculus Linear Algebra Parametric Statistics Non Parametric Statistics Probabilit y Computational \\nStatistics  Mathematical Statistics Multivariate Statistics Stochastic Process Survival Analysis Data Min ing Natural Language Processing \\nDatabases Spatial Data   Analysis  \\nWork Experience  ____________________________________________________________________________________  \\nLaboratory Assistant   FTMM Universitas Airlangga  Surabaya  Indonesia  032022  072022  \\n Laboratory Assistant for Programming Algorithm course s \\n Assisted Lecturer for scoring practice modules and mentoring for 25 students  \\n \\nProjects  _______ __________________________________________________________________________________ ___ \\nBicara Pilpres  Sentiment Analysis Dashboard for Indonesian Presidential \\nElection 2024 Candidates  Surabaya Indonesia  122022  012023  \\n Led the endtoend development process such as front end design software architecture machine learning model pipeline  \\n Successfully implemented a microservices architecture utilizing Python Flask for the main website backend and machine learni ng API to enhanc e \\nthe scalability and efficiency of the web application  Also implemented Firebase to store data using Firebase Firestore Database NoSQL and \\nFirebase Storage  \\n Implemented deployment of the web application on Digital Ocean Droplets configuring SSL certi fication and domain integration using Nginx  \\nAchieved secure and reliable web hosting ensuring optimal performance and user experience  \\n Implemented Progressive Web Application PWA for better user experience  \\n This project got Top 50 Hackfest 2023 held by  GDSC Indonesia  \\nSIBI Sistem Bahasa Isyarat  Indonesai Sign Language Alphabetic \\nClassification using Mediapipe  Surabaya Indonesia  112022  122022  \\n Implemented Tensorflow Data Generator for augmenting data to reproduce primary data  \\n Utilized the power of Mediapipe to effectively extract hand landmark points enabling the training of a cutting edge machine learning model and \\nthe development of real time detection capabilities  \\n Achieving 94 accuracy on training data and 90 accuracy on  testing data  \\nOrganizational Experience ____________________________________________________________________________  \\nLeader   GDSC Universitas Airlangga  Surabaya Indonesia  092022  Current  \\n Led a core team of 16 people and engage 500 member  \\n Held 10 workshops on data science  flutter with an average of 50 participants  \\nMinister of Science  Research and \\nTechnology   BEM FTMM Universitas \\nAirlangga  Surabaya Indonesia  012022  012023  \\n Led two sub division with 5 member each sub division  \\n Held 8 event impacting 400 FTMM students and 200 external students  \\nCore Team Mentor   GDSC Universitas Airlangga  Surabaya Indonesia  092021  072022  \\n Be a speaker at the webinar event Info session GDSC UNAIR with the topic Data Science 101 with 100 participants  \\n Made a Syllabus for workshop especially workshops that related to Data Science or Machine Learning  \\nStaff Ministry of Science \\nand Technology   BEM Universitas Airlangga  Surabaya Indonesia  092021  072022  \\n Organized Airlangga Scientific Writing Workshop and Competition impacting 100 students  \\n Organized Airlangga Scientific Forum with 32 representative participants from every faculty  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337b8232",
      "metadata": {
        "id": "337b8232"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "So, this OCR file works just fine. The OCR models predict the words with good accuracy too. But, those models used comes from DocTR pretrained model, so they are already guaranteed that the result of text extraction will be great. Self trained models will be used if they can work properly with DocTR ocr_predictor function args. Some researches may be conducted for reevaluating and remodeling to updgrade and boost accuracy for OCR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77ea7d5",
      "metadata": {
        "id": "d77ea7d5"
      },
      "outputs": [],
      "source": [
        "# Thanks"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.11 ('talentease')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "00d6bfbf0318ea386fca9a6f1e34e89dfe01dc0d94f309c9fa2c9daa7692de77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}